{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1302657f",
   "metadata": {},
   "source": [
    "# CUDA JupyterLab Setup Test (Container-Based Directory Creation)\n",
    "\n",
    "This notebook tests the CUDA JupyterLab environment and creates the necessary workspace and cache directories from within the container. This approach avoids permission issues with external storage mounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic imports\n",
    "import torch\n",
    "import transformers\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Setting up workspace and cache directories ===\")\n",
    "\n",
    "# Create workspace directory if it doesn't exist\n",
    "workspace_path = Path.home() / 'workspace'\n",
    "workspace_path.mkdir(exist_ok=True)\n",
    "print(f\"‚úì Workspace directory created/verified: {workspace_path}\")\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "cache_path = Path.home() / 'cache'\n",
    "cache_path.mkdir(exist_ok=True)\n",
    "print(f\"‚úì Cache directory created/verified: {cache_path}\")\n",
    "\n",
    "# Create HuggingFace cache subdirectory\n",
    "hf_cache_path = cache_path / 'huggingface'\n",
    "hf_cache_path.mkdir(exist_ok=True)\n",
    "print(f\"‚úì HuggingFace cache directory created/verified: {hf_cache_path}\")\n",
    "\n",
    "# Set HuggingFace cache environment variable\n",
    "os.environ['HF_HOME'] = str(hf_cache_path)\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(hf_cache_path)\n",
    "print(f\"‚úì HuggingFace cache environment variables set:\")\n",
    "print(f\"  HF_HOME: {os.environ['HF_HOME']}\")\n",
    "print(f\"  TRANSFORMERS_CACHE: {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "\n",
    "print(\"‚úì Directory setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7728419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test storage configuration (directories created from container)\n",
    "print(\"=== Storage Configuration Test ===\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Home directory: {Path.home()}\")\n",
    "\n",
    "# Check workspace (should exist after setup cell)\n",
    "workspace_path = Path.home() / 'workspace'\n",
    "print(f\"Workspace path: {workspace_path}\")\n",
    "print(f\"Workspace exists: {workspace_path.exists()}\")\n",
    "if workspace_path.exists():\n",
    "    print(f\"Workspace is writable: {os.access(workspace_path, os.W_OK)}\")\n",
    "else:\n",
    "    print(\"‚ùå Workspace directory not found - run setup cell above first\")\n",
    "\n",
    "# Check cache directory (should exist after setup cell)\n",
    "cache_path = Path.home() / 'cache'\n",
    "print(f\"Cache path: {cache_path}\")\n",
    "print(f\"Cache exists: {cache_path.exists()}\")\n",
    "if cache_path.exists():\n",
    "    print(f\"Cache is writable: {os.access(cache_path, os.W_OK)}\")\n",
    "else:\n",
    "    print(\"‚ùå Cache directory not found - run setup cell above first\")\n",
    "\n",
    "# HF cache specifically\n",
    "hf_cache = cache_path / 'huggingface'\n",
    "print(f\"HF cache path: {hf_cache}\")\n",
    "print(f\"HF cache exists: {hf_cache.exists()}\")\n",
    "\n",
    "# Check environment variables\n",
    "print(f\"HF_HOME environment variable: {os.environ.get('HF_HOME', 'Not set')}\")\n",
    "print(f\"TRANSFORMERS_CACHE environment variable: {os.environ.get('TRANSFORMERS_CACHE', 'Not set')}\")\n",
    "\n",
    "print(\"‚úì All directories created from within container successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hugging Face transformers with local cache\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Testing Hugging Face model download (cache on disk B)...\")\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "try:\n",
    "    # This will download to the cache directory on disk B\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Test basic functionality\n",
    "    test_text = \"Hello, this is a test of the Hugging Face integration with disk B storage!\"\n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        print(\"‚úì Model moved to GPU\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    print(f\"‚úì Model loaded and tested successfully!\")\n",
    "    print(f\"‚úì Output shape: {outputs.last_hidden_state.shape}\")\n",
    "    print(f\"‚úì Model cached on disk B for future use\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ba84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test flash attention if available\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(f\"‚úì Flash Attention available: {flash_attn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Flash Attention not available (this is normal for some setups)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU memory and performance\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=== GPU Memory Test ===\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check initial memory\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Create a tensor\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    y = torch.randn(1000, 1000, device='cuda')\n",
    "    \n",
    "    print(f\"After tensor creation: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Perform operation\n",
    "    z = torch.matmul(x, y)\n",
    "    \n",
    "    print(f\"After matrix multiplication: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "    print(f\"‚úì GPU computation successful!\")\n",
    "    \n",
    "    # Clean up\n",
    "    del x, y, z\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"After cleanup: {torch.cuda.memory_allocated() / 1024**2:.1f} MB\")\n",
    "else:\n",
    "    print(\"‚ùå GPU not available for memory test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b147e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"        SETUP TEST SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚úì JupyterLab running with NVIDIA PyTorch container\")\n",
    "print(f\"‚úì Python {sys.version.split()[0]}\")\n",
    "print(f\"‚úì PyTorch {torch.__version__}\")\n",
    "print(f\"‚úì Transformers {transformers.__version__}\")\n",
    "print(f\"‚úì CUDA: {'Available' if torch.cuda.is_available() else 'Not Available'}\")\n",
    "print(f\"‚úì Workspace and cache directories created from container\")\n",
    "print(f\"‚úì HF cache environment variables configured\")\n",
    "print(f\"‚úì HF models cache on external storage (via container)\")\n",
    "print(f\"‚úì No external folder creation required!\")\n",
    "print(\"\\nüéâ Environment ready for ML/DL development!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
